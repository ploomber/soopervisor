.. _k8s-gc:

Google Cloud
------------

.. important:: This tutorial requires soopervisor ``0.6.1`` or higher

This second tutorial runs a pipeline in a local Kubernetes cluster using Google Cloud.


.. note:: You may use or create a new `Google Cloud project <https://cloud.google.com/resource-manager/docs/creating-managing-projects>`_ to follow this tutorial.

Pre-requisites
**************

* ``kubectl``
* `Google Cloud SDK <https://cloud.google.com/sdk/docs/install>`_
* `conda instructions <https://docs.conda.io/en/latest/miniconda.html>`_
* `git <https://git-scm.com/book/en/v2/Getting-Started-Installing-Git>`_
* Install Ploomber with ``pip install ploomber``

Instructions
************

Create a cluster and install Argo:

.. code-block:: sh

    # create cluster
    gcloud container clusters create my-cluster --num-nodes=1 --zone us-east1-b

    # install argo
    kubectl create ns argo
    kubectl apply -n argo -f https://raw.githubusercontent.com/argoproj/argo-workflows/stable/manifests/quick-start-postgres.yaml

    # create storage bucket (choose whatever name you want)
    gsutil mb gs://YOUR-BUCKET-NAME


Submit a sample workflow to make sure Argo is working:

.. code-block:: sh

    argo submit -n argo --watch https://raw.githubusercontent.com/argoproj/argo/master/examples/hello-world.yaml


.. tip:: Enable Argo's UI:

    .. code-block:: sh

        # port forwarding to enable the UI
        kubectl -n argo port-forward svc/argo-server 2746:2746

    Then, open: https://127.0.0.1:2746

Install ``ploomber``:

.. code-block:: sh

    pip install ploomber

Let's now run a Ploomber sample Machine Learning pipeline:

.. code-block:: sh

    # get example
    ploomber examples -n templates/ml-online -o ml-online
    cd ml-online

    # configure development environment
    ploomber install

    # activate environment
    conda activate ml-online

    # add a new target platform
    soopervisor add training --backend argo-workflows


The previous command creates a ``soopervisor.yaml`` file where we can configure
the container registry to upload our Docker image:

.. code-block:: yaml

    training:
      backend: argo-workflows
      repository: gcr.io/PROJECT-ID/my-ploomber-pipeline

Replace ``PROJECT-ID`` with your actual project ID.

Each task will run in isolation, we must ensure that products generated by
a given task are available to its corresponding downstream tasks. Ww can use
Google Cloud Storage for that, add the following to the
``src/ml_online/pipeline.yaml`` file:

.. code-block:: yaml

    # more content above...

    serializer: ml_online.io.serialize
    unserializer: ml_online.io.unserialize

    # add these two lines
    clients:
      File: ml_online.clients.get_gcloud
    
    # content continues...

The previous change tells Ploomber to call the function ``get_gcloud`` defined
in module ``src/ml_online/clients.py`` to get the client. Edit the
``clients.py`` to add your bucket name:


.. code-block:: python

    from ploomber.clients import GCloudStorageClient

    def get_gcloud():
        # edit YOUR-BUCKET-NAME
        return GCloudStorageClient(bucket_name='YOUR-BUCKET-NAME',
                                   parent='ml-online',
                                   json_credentials_path='credentials.json')

You can ignore the rest of the file. Finally, we add service account credentials to
upload to Google Cloud Storage. To learn more about service accounts,
`click here <https://cloud.google.com/docs/authentication/production>`_.


Store the service account details in a ``credentials.json`` in the root project
directory (same folder as ``setup.py``):

We are ready to execute the workflow:

.. code-block:: sh

    # authenticate to push docker image
    gcloud auth configure-docker

    # packages code, create docker image and upload it (takes a few mins)
    soopervisor export training

    # submit workflow
    argo submit -n argo training/argo.yaml


You may keep track of execution by opening the UI. Check out the bucket to see output.

**Congratulations! You just ran Ploomber on Kubernetes!**


.. note::

    You may fail to submit pipeline with a different example (e.g. ``ml-basic``). That is because we used the ``ml-intermediate`` pipeline as the example, which already has parametrized products. Therefore, we need to add the ``env.yaml`` and parameterize the pipeline to run it successfully.

    However, the ``ml-basic`` pipeline was not parametrized. Thus we need to parameterize it first. Please refer to `this documentation <https://docs.ploomber.io/en/latest/user-guide/parametrized.html>`_ for more information on parametrized pipelines.


.. attention:: 

    Make sure you delete your cluster, bucket, and image after running this example!

    .. code-block:: sh

        # delete cluster
        gcloud container clusters delete my-cluster --zone us-east1-b

        # delete bucket
        gsutil rm -r gs://my-sample-ploomber-bucket

        # delete image (you can get the image id from the google cloud console)
        gcloud container images delete IMAGE-ID


Optional: Mounting a shared disk
********************************

.. note::

    If you use a shared disk instead of storing artifacts in S3 or Google Cloud
    Storage, you must execute the pipeline with the ``--skip-tests`` flag. e.g.,
    ``soopervisor export training --skip-tests``, otherwise the command will
    fail if your project does not have a remote storage client configured.

In the example, we configured the ``pipeline.yaml`` file to use Google Cloud
Storage to store artifacts, this serves two purposes: 1) Make artifacts
available to us upon execution, and 2) Make artifacts available to dowstream
tasks.

This happens because pods run in isolation, if task B depends on task A, it
will fetch A's output from cloud storage before execution. We can save dowload
time (and cut costs) by mounting a shared volume so that B doesn't have to
download A's output. Ploomber automatically detects this change and only calls
the cloud storage API for uploading.

Here's how to configure a shared disk:

.. code-block:: sh

    # create disk. make sure the zone matches your cluster
    gcloud compute disks create --size=10GB --zone=us-east1-b gce-nfs-disk

    # configure the nfs server
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/01-nfs-server.yaml
    kubectl apply -f 01-nfs-server.yaml

    # create service
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/02-nfs-service.yaml
    kubectl apply -f 02-nfs-service.yaml

    # check service
    kubectl get svc nfs-server

    # create persistent volume claim
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/03-nfs-pv-pvc.yaml
    kubectl apply -f 03-nfs-pv-pvc.yaml


**Optionally**, you can check that the disk is properly configured by running this sample workflow:

.. code-block:: sh

    # run sample workflow (uses nfs and creates an empty file on it)
    curl -O https://raw.githubusercontent.com/ploomber/soopervisor/master/doc/assets/dag.yaml
    argo submit -n argo --watch dag.yaml


Check the output:

.. code-block:: sh

    # get nfs-server pod name
    kubectl get pod

    # replace with the name of the pod
    kubectl exec --stdin --tty {nfs-server-pod-name} -- /bin/bash

Once inside the Pod, run:

.. code-block:: sh

    ls /exports/

You should see files A, B, C, D. Generated by the previous workflow.


Let's now run the Machine Learning workflow. Since we configured a shared disk,
artifacts from upstream tasks will be available to downstream ones (no need
to download them from Cloud Storage anymore); the Cloud Storage client is only used to upload
artifacts for us to review later.

To make the shared disk available to the pods that run each task, we have
to modify ``soopervisor.yaml``:

.. code-block:: yaml

    training:
      backend: argo-workflows
      repository: gcr.io/your-project/your-repository
      mounted_volumes:
        - name: nfs
          sub_path: my-shared-folder
          spec:
            persistentVolumeClaim:
              claimName: nfs

This exposes ``/my-shared-folder`` sub directory in our shared disk
in ``/mnt/nfs/`` on each pod. Now, we must configure the pipeline to store all
products in ``/mnt/nfs/``. Create an ``env.yaml`` file in the root folder
(same folder that contains the ``setup.py`` file) with this content:


.. code-block:: yaml

    sample: False
    # this configures the pipeline to store all outputs in the shared disk
    product_root: /mnt/nfs
